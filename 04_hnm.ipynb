{"cells":[{"cell_type":"markdown","metadata":{"collapsed":false,"id":"I8F3XGz_dyXc"},"source":["# Deep Semantic Search for Hyperpersonalized Recommendations"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"cmjr1dz8dyXd"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{},"source":["You can get the original code and data from zach-blumenfeld's [Neo4j Generative AI Workshop](https://github.com/neo4j-product-examples/genai-workshop/tree/main)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OOR5YnnkdyXd"},"outputs":[],"source":["%%capture\n","%pip install sentence_transformers langchain openai tiktoken python-dotenv gradio graphdatascience altair\n","%pip install \"vegafusion[embed]\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7psF1otOdyXe"},"outputs":[],"source":["from graphdatascience import GraphDataScience\n","from dotenv import load_dotenv\n","import os\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime, timedelta\n","from langchain.embeddings import OpenAIEmbeddings, BedrockEmbeddings, SentenceTransformerEmbeddings"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"_ar1ZFhPdyXe"},"source":["### Setup Credentials and Environment Variables\n","\n","To make this easy, you can write the credentials and env variables directly into the below cell\n","\n","If you like you can use an environments file instead by copying `ws.env.template` to `ws.env` and filling credentials and variables in there. This is a best practice for the future, but fine to skip for this workshop."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-98NuINdyXe"},"outputs":[],"source":["# You can skip this if not using a ws.env file\n","if os.path.exists('ws.env'):\n","    load_dotenv('ws.env', override=True)\n","\n","    # Neo4j\n","    NEO4J_URI = os.getenv('NEO4J_URI')\n","    NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n","    NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n","    AURA_DS = False\n","\n","    # AI\n","    EMBEDDING_MODEL = 'openai'\n","    LLM = 'gpt-3.5'"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"yU13AU73dyXf"},"source":["### Connect to Neo4j"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92GFeMaRdyXf"},"outputs":[],"source":["# Use Neo4j URI and credentials according to our setup\n","gds = GraphDataScience(\n","    NEO4J_URI,\n","    auth=(NEO4J_USERNAME, NEO4J_PASSWORD),\n","    aura_ds=AURA_DS)\n","\n","# Necessary if you enabled Arrow on the db - this is true for AuraDS\n","gds.set_database(\"hnm\")"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"SpKcGgRZdyXg"},"source":["## Vector Search\n","In this Section We will build Text Embeddings of Product and demonstrate how to leverage the Neo4j vector index for vector search."]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"-WrlFCN1dyXg"},"source":["### Creating Text Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_LGUbVS1dyXg"},"outputs":[],"source":["def load_embedding_model(embedding_model_name: str):\n","    if embedding_model_name == \"openai\":\n","        embeddings = OpenAIEmbeddings()\n","        dimension = 1536\n","        print(\"Embedding Model: openai\")\n","    elif embedding_model_name == \"aws\":\n","        embeddings = BedrockEmbeddings()\n","        dimension = 1536\n","        print(\"Embedding Model: aws\")\n","    else:\n","        embeddings = SentenceTransformerEmbeddings(\n","            model_name=\"all-MiniLM-L6-v2\", cache_folder=\"/embedding_model\")\n","        print(\"Embedding Model: sentence transformer\")\n","        dimension = 384\n","    return embeddings, dimension"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1041,"status":"ok","timestamp":1699870634177,"user":{"displayName":"Kristof Neys","userId":"09852078574024706766"},"user_tz":-60},"id":"KwRdCBawdyXg","outputId":"9adf44a4-1076-4c55-e242-8108a31ae68c"},"outputs":[],"source":["embedding_model, dimension = load_embedding_model(EMBEDDING_MODEL)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"rGoMu3bqekeE"},"source":["### Vector Search Using Cypher"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20930,"status":"ok","timestamp":1699871604875,"user":{"displayName":"Kristof Neys","userId":"09852078574024706766"},"user_tz":-60},"id":"elNIp0_BdyXo","outputId":"b262f4cc-0ea1-45a4-a90c-e3a9b7c5da88"},"outputs":[],"source":["#search_prompt = 'denim jeans, loose fit, high-waist'\n","search_prompt = input() #'Oversized Sweaters'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":526,"status":"ok","timestamp":1699871613268,"user":{"displayName":"Kristof Neys","userId":"09852078574024706766"},"user_tz":-60},"id":"H6uSK26UdyXp","outputId":"5a60cbdd-0b41-4273-bc72-7b93897b891b"},"outputs":[],"source":["query_vector = embedding_model.embed_query(search_prompt)\n","print(f'query vector length: {len(query_vector)}')\n","print(f'query vector sample: {query_vector[:10]}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.set_option('display.max_rows', 10)\n","pd.set_option('display.max_colwidth', 500)\n","pd.set_option('display.width', 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":520},"executionInfo":{"elapsed":1228,"status":"ok","timestamp":1699871751844,"user":{"displayName":"Kristof Neys","userId":"09852078574024706766"},"user_tz":-60},"id":"pRTY5LnPdyXp","outputId":"0fab2ab7-0ff9-48ca-a31a-1214644cb5f1"},"outputs":[],"source":["gds.run_cypher('''\n","CALL db.index.vector.queryNodes(\"product-text-embeddings\", 10, $queryVector)\n","YIELD node AS product, score\n","RETURN product.productCode AS productCode,\n","    product.text AS text,\n","    score\n","''', params={'queryVector': query_vector})"]},{"cell_type":"markdown","metadata":{"id":"RLmyZ_5lhTRm"},"source":["### Vector Search Using Langchain\n","\n","We can also do this with langchain which is a recommended approach going forward.  To do this we use the Neo4jVector class and call the method to sert it up from an existing index in the graph."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fv3tuEteekeF"},"outputs":[],"source":["from langchain.vectorstores.neo4j_vector import Neo4jVector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ofoi5aJvekeF"},"outputs":[],"source":["kg_vector_search = Neo4jVector.from_existing_index(\n","    embedding=embedding_model,\n","    url=NEO4J_URI,\n","    username=NEO4J_USERNAME,\n","    password=NEO4J_PASSWORD,\n","    database='hnm',\n","    index_name='product-text-embeddings')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":962,"status":"ok","timestamp":1699871866297,"user":{"displayName":"Kristof Neys","userId":"09852078574024706766"},"user_tz":-60},"id":"MDFoMXmbekeF","outputId":"7c6cc32b-6549-45c7-9b06-988585b49e2f"},"outputs":[],"source":["res = kg_vector_search.similarity_search(search_prompt, k=10)\n","res"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":296,"status":"ok","timestamp":1699871875663,"user":{"displayName":"Kristof Neys","userId":"09852078574024706766"},"user_tz":-60},"id":"qQP7e2dcekeF","outputId":"d0ca7fdf-70e4-4493-d3df-3fc07a622c99"},"outputs":[],"source":["# Visualize as a dataframe\n","pd.DataFrame([{'document': d.page_content} for d in res])"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"IXn_6bFCekeF"},"source":["### Try Yourself"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":503},"executionInfo":{"elapsed":626,"status":"ok","timestamp":1699871930542,"user":{"displayName":"Kristof Neys","userId":"09852078574024706766"},"user_tz":-60},"id":"rHayKAOlekeF","outputId":"877484c3-a61a-4cf7-953e-1a5c3ddfb75e"},"outputs":[],"source":["res = kg_vector_search.similarity_search('red sweather', k=10)\n","pd.DataFrame([{'document': d.page_content} for d in res])"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"tofjZIsSekeF"},"source":["## Semantic Search with Context\n","Using Explicit Relationships in enterprise data"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"JIdx4DiXekeF"},"source":["\n","Above we see how you can use the vector index to find semantic similar products in user searches.  but there is a rich graph full of other information in it. Lets leverage our knowledge graph to make this better"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"QdiPqIq2dyXp"},"source":["An important piece of information expressed in this graph, but not directly in the documents, is customer purchasing behavior.  We can use A Cypher Query to make recommendations without any document behavior. this is similar to collaborative filtering but generalized to purchase history (not necessarily rating based)"]},{"cell_type":"markdown","metadata":{"id":"G4ghIv8miXrt"},"source":["#### Example Purchase History"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"DQCdy6jXdyXp"},"source":["Consider the below customer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":788},"executionInfo":{"elapsed":1243,"status":"ok","timestamp":1699872016939,"user":{"displayName":"Kristof Neys","userId":"09852078574024706766"},"user_tz":-60},"id":"zSTgu7YrdyXp","outputId":"589d8dc0-246e-4da2-84bb-0d28a2d9e242"},"outputs":[],"source":["CUSTOMER_ID = \"daae10780ecd14990ea190a1e9917da33fe96cd8cfa5e80b67b4600171aa77e0\"\n","print('Customer Purchase History')\n","gds.run_cypher('''\n","    MATCH(c:Customer {customerId: $customerId})-[:PURCHASED]->(:Article)\n","    -[:VARIANT_OF]->(p:Product)\n","    RETURN p.productCode AS productCode,\n","        p.prodName AS prodName,\n","        p.productTypeName AS productTypeName,\n","        p.garmentGroupName AS garmentGroupName,\n","        p.detailDesc AS detailDesc,\n","        count(*) AS purchaseCount\n","    ORDER BY purchaseCount DESC\n","''', params={'customerId': CUSTOMER_ID})"]},{"cell_type":"markdown","metadata":{"id":"2o8X9NX9igPm"},"source":["#### Graph Patterns For Retrieval Query"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":470},"executionInfo":{"elapsed":1671,"status":"ok","timestamp":1699872078357,"user":{"displayName":"Kristof Neys","userId":"09852078574024706766"},"user_tz":-60},"id":"J1YY_a99dyXp","outputId":"7726e656-d078-44a3-bd9f-51ae1521f6fe"},"outputs":[],"source":["# This is the example Pattern we can use to predict likely customer preferences based on collaborative behavior\n","gds.run_cypher('''\n","    MATCH(c:Customer {customerId: $customerId})-[:PURCHASED]->(:Article)\n","    <-[:PURCHASED]-(:Customer)-[:PURCHASED]->(:Article)\n","    -[:VARIANT_OF]->(p:Product)\n","    RETURN p.productCode AS productCode,\n","        p.prodName AS prodName,\n","        p.productTypeName AS productTypeName,\n","        p.garmentGroupName AS garmentGroupName,\n","        p.detailDesc AS detailDesc,\n","        count(*) AS score\n","    ORDER BY score DESC LIMIT 10\n","''', params={'customerId': CUSTOMER_ID})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8eKwXKf_ekeF"},"outputs":[],"source":["# This is the example Pattern we can use to predict likely customer preferences based on collaborative behavior\n","# Finds products that match the search and has been purchased by customers with same purchase history as customer XXX\n","\n","kg_personalized_search = Neo4jVector.from_existing_index(\n","    embedding=embedding_model,\n","    url=NEO4J_URI,\n","    username=NEO4J_USERNAME,\n","    password=NEO4J_PASSWORD,\n","    database='hnm',\n","    index_name='product-text-embeddings',\n","    retrieval_query=f\"\"\"\n","    WITH node AS product, score AS searchScore\n","\n","    OPTIONAL MATCH(product)<-[:VARIANT_OF]-(:Article)<-[:PURCHASED]-(:Customer)\n","    -[:PURCHASED]->(a:Article)<-[:PURCHASED]-(:Customer {{customerId: '{CUSTOMER_ID}'}})\n","\n","    WITH count(a) AS purchaseScore, product.text AS text, searchScore, product.productCode AS productCode\n","    RETURN text,\n","        (1+purchaseScore)*searchScore AS score,\n","        {{productCode: productCode, purchaseScore:purchaseScore, searchScore:searchScore}} AS metadata\n","    ORDER BY purchaseScore DESC, searchScore DESC LIMIT 15\n","    \"\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":710},"executionInfo":{"elapsed":2679,"status":"ok","timestamp":1699872219400,"user":{"displayName":"Kristof Neys","userId":"09852078574024706766"},"user_tz":-60},"id":"rUCSQyrZekeF","outputId":"f6fc1f3b-9426-4f59-ee05-a04919027258"},"outputs":[],"source":["res = kg_personalized_search.similarity_search(search_prompt, k=100)\n","\n","# Visualize as a dataframe\n","pd.DataFrame([{'productCode': d.metadata['productCode'],\n","               'document': d.page_content,\n","               'searchScore': d.metadata['searchScore'],\n","               'purchaseScore': d.metadata['purchaseScore']} for d in res])"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"t55maEsfdyXp"},"source":["## KG Powered Inference for AI\n","\n","We saw before how could use graph pattern matching to personalize search and make it more relevant.\n","\n","Graph pattern matching is very power and can work well in a lot of scenarios.\n","\n","In addition to this, we also have Graph Data Science, which can allow as to enrich the current Knowledge graph with machine learning, that can\n","1. Provide addition information to improve relevancy of search results at scale\n","2. Provide additional inferences to GenAI\n","\n","We will show an example of how this works using Node Embedding and K-Nearest Neighbor algorithms\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"gm0M6TsidyXp"},"source":["### Graph Embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pfSOEBrLdyXp"},"outputs":[],"source":["pd.set_option('display.max_rows', 20)\n","pd.set_option('display.max_colwidth', 500)\n","pd.set_option('display.width', 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OKIFwRlydyXp"},"outputs":[],"source":["def clear_all_graphs():\n","    g_names = gds.graph.list().graphName.tolist()\n","    for g_name in g_names:\n","        g = gds.graph.get(g_name)\n","        g.drop()"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"fzwUJMfYdyXp"},"source":["#### Clear Past Analysis (If rerunning this Notebook)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRN0RJuZdyXp"},"outputs":[],"source":["clear_all_graphs()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"WhnX3y1kdyXp","outputId":"b7f056c5-a0ca-4f53-e8e5-06349366f1a0"},"outputs":[],"source":["gds.run_cypher('''\n","    MATCH(:Article)-[r:CUSTOMERS_ALSO_LIKE]->()\n","    CALL {\n","        WITH r\n","        DELETE r\n","    } IN TRANSACTIONS OF 1000 ROWS\n","    ''')"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"NAgt16h-dyXp"},"source":["#### Apply Fast Random Projection Node Embedding"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"OH74FkdidyXp"},"source":["First, apply a graph projection to structure the portion of the graph we need in an optimized in-memory format for graph ML."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26533,"status":"ok","timestamp":1699872718557,"user":{"displayName":"Kristof Neys","userId":"09852078574024706766"},"user_tz":-60},"id":"PtGT6HfgekeG","outputId":"27243335-88d6-41e8-9415-36d340c8cff1"},"outputs":[],"source":["%%time\n","\n","# graph projection\n","gds.run_cypher('''\n","   MATCH (a1:Article)<-[:PURCHASED]-(:Customer)-[:PURCHASED]->(a2:Article)\n","   WITH gds.graph.project(\"proj\", a1, a2,\n","       {sourceNodeLabels: labels(a1),\n","       targetNodeLabels: labels(a2),\n","       relationshipType: \"COPURCHASE\"}) AS g\n","   RETURN g.graphName\n","   ''')\n","\n","g = gds.graph.get(\"proj\")"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"g9fF7LjPdyXq"},"source":["Next, we will generate node embeddings for similarity calculation.  In this case, we will use FastRP (Fast Random Projection) which is a fast, scalable, and robust embedding algorithm. FastRP calculates embeddings using probabilistic sampling and linear algebra."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171,"referenced_widgets":["5b5a261ce52649759538b06b1d00ec0d","43ee75db1a1a486bb944c7931f5d9160","ef3ac03f92b44465a26d51cd62d816eb","1c59e75d027d47cb96343f1af89dd1b1","65dbcea2dfba4bd7be50a9e76b65d086","3e83412ca7c1496096af0ec386ff7aac","be48419a3ad34da6877b78554f009bc4","409783c7087440c980e05fb9a33cca74","dc033bf8a9b44b2692051ecd0c980881","e439939467c14b458d1d1fd8c5289696","1276e5eefd774760ac26905b5b77c869"]},"executionInfo":{"elapsed":6439,"status":"ok","timestamp":1699872750725,"user":{"displayName":"Kristof Neys","userId":"09852078574024706766"},"user_tz":-60},"id":"vhcVnqWvdyXq","outputId":"f60f8580-2262-412b-aafc-0e646a40fdfd"},"outputs":[],"source":["%%time\n","# embeddings (writing back Article embeddings in case we want to introspect later)\n","gds.fastRP.mutate(g, mutateProperty='embedding', embeddingDimension=128, randomSeed=7474, concurrency=4, iterationWeights=[0.0, 1.0, 1.0])\n","gds.graph.writeNodeProperties(g, ['embedding'], ['Article'])"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"Zu6MWuaDekeG"},"source":["#### Explore Node Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"35pfsqVwdyXq"},"outputs":[],"source":["graph_emb_df = gds.run_cypher('''\n","MATCH (p:Product)<-[:VARIANT_OF]-(a:Article)-[:FROM_DEPARTMENT]-(d)\n","RETURN a.articleId AS articleId,\n","    p.prodName AS productName,\n","    p.productTypeName AS productTypeName,\n","    d.departmentName AS departmentName,\n","    d.sectionName AS sectionName,\n","    p.detailDesc AS detailDesc,\n","    a.embedding AS embedding\n","''')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":525},"executionInfo":{"elapsed":257,"status":"ok","timestamp":1699872802591,"user":{"displayName":"Kristof Neys","userId":"09852078574024706766"},"user_tz":-60},"id":"8Qcq2Z_XekeG","outputId":"8b5345bb-a885-49d1-b36a-8542492abfc1"},"outputs":[],"source":["graph_emb_df[:3]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":161350,"status":"ok","timestamp":1699872995694,"user":{"displayName":"Kristof Neys","userId":"09852078574024706766"},"user_tz":-60},"id":"eNXMIDaGekeG","outputId":"003ec121-3808-42d0-bc34-705fc40462d7"},"outputs":[],"source":["# Takes 30sec to run\n","from sklearn.manifold import TSNE\n","\n","df = graph_emb_df.copy()\n","filtered_node_df = df[df.embedding.apply(lambda x: np.count_nonzero(x) > 0)].reset_index(drop=True)\n","# instantiate the TSNE model\n","tsne = TSNE(n_components=2, random_state=7474, init='random', learning_rate=\"auto\")\n","# Use the TSNE model to fit and output a 2-d representation\n","E = tsne.fit_transform(np.stack(filtered_node_df['embedding'], axis=0))\n","\n","coord_df = pd.concat([filtered_node_df, pd.DataFrame(E, columns=['x', 'y'])], axis=1)\n","coord_df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":794},"id":"7yNBed6eekeG","outputId":"e98403e3-3394-42df-abed-b98e4b29f539"},"outputs":[],"source":["import altair as alt\n","from sklearn.manifold import TSNE\n","\n","alt.data_transformers.disable_max_rows()\n","chart = alt.Chart(coord_df.sample(n=5000, random_state=7474)).mark_circle(size=60).encode(\n","    x='x',\n","    y='y',\n","    tooltip=['productName', 'productTypeName', 'departmentName' , 'sectionName', 'detailDesc']\n",").properties(title=\"Article Embedding (2D Representation)\", width=750, height=700)\n","\n","chart = chart.configure_axis(titleFontSize=20)\n","chart.configure_legend(labelFontSize = 20)\n","chart"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"5IKxoklYdyXq"},"source":["### K-Nearest Neighbors (KNN) Relationships\n","\n","Finally, we can do our similarity inference with K-Nearest Neighbor (KNN) and write back to the graph.\n","We will use a slightly low cutoff of 0.75 similarity score to extend the result size for exploration.  We can provide a higher cutoff at query time if needed."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":552,"referenced_widgets":["225085d2e10b470585295d2874fdddbd","d080ad2952e5484ba7a1d5d316b23937","3cdc91c7a77d4e0d877c30497c69f207","32657aa7bcaa4cc3aec4317a5a9cbae1","fdf98d0b55e649b9882f530e299b76a7","d35230e08e8a4cec97bcbbd603d660a5","87e17419744f42eb9ecbf995ec5cde4c","f29d15f0ee2b41fcb92f80b7fa2ea944","e920f4310fcf422dbe9300db0bd361d7","8deecfeaf21444d69725d14ab763805c","443c064fee954412a928de68c37fd7d0"]},"executionInfo":{"elapsed":30940,"status":"ok","timestamp":1699873043745,"user":{"displayName":"Kristof Neys","userId":"09852078574024706766"},"user_tz":-60},"id":"bpuDKk6hdyXq","outputId":"a3205915-ed2b-4eb2-e08f-a4146ca347d5"},"outputs":[],"source":["%%time\n","# KNN\n","_ = gds.knn.write(g, nodeProperties=['embedding'], nodeLabels=['Article'],\n","                  writeRelationshipType='CUSTOMERS_ALSO_LIKE', writeProperty='score',\n","                  sampleRate=1.0, initialSampler='randomWalk', concurrency=1, similarityCutoff=0.75, randomSeed=7474)\n","_"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZP3C-w1fdyXq","outputId":"ecbc63c9-6367-44c0-9500-94ced8098cc9"},"outputs":[],"source":["# clear graph projection once done\n","g.drop()"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"7nNobdJhekeH"},"source":["### Tailored Recommendations from Search\n","\n","Let's construct a KG store to retrieve recommendations based on search"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVy9J6T5ekeH"},"outputs":[],"source":["# Based on a search, what other product should we recommend?\n","kg_search_recommendations = Neo4jVector.from_existing_index(\n","    embedding=embedding_model,\n","    url=NEO4J_URI,\n","    username=NEO4J_USERNAME,\n","    password=NEO4J_PASSWORD,\n","    database='hnm',\n","    index_name='product-text-embeddings',\n","    retrieval_query=\"\"\"\n","    WITH node as searchProduct, score as searchScore\n","    MATCH(searchProduct)<-[:VARIANT_OF]-(:Article)-[r:CUSTOMERS_ALSO_LIKE]->(:Article)-[:VARIANT_OF]-(product)\n","    WITH  product, searchScore, sum(r.score*searchScore) AS recommenderScore\n","    RETURN product.text AS text,\n","    recommenderScore AS score,\n","    {productCode: product.productCode, productType: product.productTypeName, recommenderScore:recommenderScore} AS metadata\n","    ORDER BY score DESC LIMIT 100\n","    \"\"\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":667},"executionInfo":{"elapsed":1786,"status":"ok","timestamp":1699873190041,"user":{"displayName":"Kristof Neys","userId":"09852078574024706766"},"user_tz":-60},"id":"1M3YlaOgekeH","outputId":"72fba06c-7816-4c1e-f4fe-f2b5dcf4b18f"},"outputs":[],"source":["res = kg_search_recommendations.similarity_search(search_prompt, k=100)\n","\n","# Visualize as a dataframe\n","pd.DataFrame([{'productCode': d.metadata['productCode'],\n","               'productType':d.metadata['productType'],\n","               'document': d.page_content,\n","               'recommenderScore': d.metadata['recommenderScore']} for d in res])"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"gvenRCwaekeH"},"source":["### Personalized Recommendations\n","\n","N ow lets look at personalized recommendations for a bit.  To keep things simple we will base this just on purchase history, not search, though we could do both if we wanted to (similar to what we did in the above Semantic Search with context section)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"MU--XH9nekeH"},"source":["First, we will start by creating a Neo4jGraph object which we can then query. This is different from the vec tor based retrievers above"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"taBhqQWPekeH"},"outputs":[],"source":["from langchain.graphs import Neo4jGraph\n","\n","kg = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database='hnm')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"fBZCfWZnekeH","outputId":"11fcdc67-7d19-46f6-afc3-2869f1ec6c9d"},"outputs":[],"source":["# Looking at this customer purchase history, what ptoduct could be recommended to him?\n","res = kg.query('''\n","    MATCH(:Customer {customerId:$customerId})-[:PURCHASED]->(:Article)\n","    -[r:CUSTOMERS_ALSO_LIKE]->(:Article)-[:VARIANT_OF]->(product)\n","    RETURN product.productCode AS productCode,\n","        product.prodName AS prodName,\n","        product.productTypeName AS productType,\n","        product.text AS document,\n","        sum(r.score) AS recommenderScore\n","    ORDER BY recommenderScore DESC LIMIT $k\n","    ''', params={'customerId': CUSTOMER_ID, 'k':15})\n","\n","#visualize as dataframe. result is list of dict\n","pd.DataFrame(res)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"Vk4JNeMWekeH"},"source":["We could also make it based of the latest purchases.  For example consider the last purchase for the customer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":98},"id":"schZD5K5ekeH","outputId":"cb828f5a-e52c-45ff-c778-3c27dd2bd6ab"},"outputs":[],"source":["# last two purchases\n","pd.DataFrame(kg.query('''\n","    MATCH(:Customer {customerId:$customerId})-[t:PURCHASED]->(:Article)-[:VARIANT_OF]->(product)\n","    RETURN product.productCode AS productCode,\n","        product.prodName AS prodName,\n","        product.productTypeName AS productType,\n","        product.text AS document,\n","        t.tDat as purchaseDate\n","    ORDER BY purchaseDate DESC LIMIT $k\n","    ''', params={'customerId': CUSTOMER_ID, 'k':3}))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"w3T698NEekeH","outputId":"d835f238-2b01-4558-8b7e-bf3b32e981ef"},"outputs":[],"source":["# Based on the last 20 purchase, what product should we recommedn to this particular customer?\n","res = kg.query('''\n","    MATCH(:Customer {customerId:$customerId})-[t:PURCHASED]->(a:Article)\n","    WITH a, t.tDat as purchaseDate\n","    ORDER BY purchaseDate DESC LIMIT $lastNPurchases\n","    MATCH(a)-[r:CUSTOMERS_ALSO_LIKE]->(:Article)-[:VARIANT_OF]->(product)\n","    RETURN product.productCode AS productCode,\n","        product.prodName AS prodName,\n","        product.productTypeName AS productType,\n","        product.text AS document,\n","        sum(r.score) AS recommenderScore\n","    ORDER BY recommenderScore DESC LIMIT $k\n","    ''', params={'customerId': CUSTOMER_ID, 'lastNPurchases':20, 'k':15})\n","\n","#visualize as dataframe. result is list of dict\n","pd.DataFrame(res)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"Hb5TSaKEekeH"},"source":["The same could be done with other filters, such as on transaction date."]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"IqP4PJtRekeH"},"source":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"90E9HGu4dyXq"},"source":["## LLM For Generating Grounded Content\n","\n","Let's use an LLM to automatically generate content for targeted marketing campaigns grounded with our knowledge graph using the above tools.\n","Here is a quick example for generating promotional messages. but you can create all sorts of content with this!\n","\n","For our first message, let's consider a scenario where a user recently searched for products, but perhaps didn't commit to a purchase yet. We now want to send a message to promote relevant products."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JI9LVEdKekeH"},"outputs":[],"source":["# Import relevant libraries\n","from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n","from langchain.chat_models import ChatOpenAI\n","from langchain.schema import StrOutputParser"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YuCHAuTrdyXr","outputId":"e793ad37-5e91-44e6-f4f4-38dc622f0275"},"outputs":[],"source":["#load LLM\n","\n","def load_llm(llm_name: str):\n","    if llm_name == \"gpt-4\":\n","        print(\"LLM: Using GPT-4\")\n","        return ChatOpenAI(temperature=0, model_name=\"gpt-4\", streaming=True)\n","    elif llm_name == \"gpt-3.5\":\n","        print(\"LLM: Using GPT-3.5\")\n","        return ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True)\n","    elif llm_name == \"claudev2\":\n","        print(\"LLM: ClaudeV2\")\n","        return BedrockChat(\n","            model_id=\"anthropic.claude-v2\",\n","            model_kwargs={\"temperature\": 0.0, \"max_tokens_to_sample\": 1024},\n","            streaming=True,\n","        )\n","    print(\"LLM: Using GPT-3.5\")\n","    return ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True)\n","\n","\n","llm = load_llm(LLM)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"8I6JesV0ekeH"},"source":["### Create Knowledge Graph Stores for Retrieval\n","To ground our content Generation we need to define retrievers to pull information from our knowledge graph.  Let's make two stores:\n","1. Personalized Search Retriever (`kg_personalized_search`): Based on recent customer searches and purchase history, pull relevant products\n","2. Recommendations retriever (`kg_recommendations`): Based on recent customer searches, what else may we recommend to them?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLBBVRXwdyXq"},"outputs":[],"source":["# This will be a function so we can change per customer id\n","# We will use a mock URL for our sources in the metadata\n","def kg_personalized_search_gen(customer_id):\n","    return Neo4jVector.from_existing_index(\n","        embedding=embedding_model,\n","        url=NEO4J_URI,\n","        username=NEO4J_USERNAME,\n","        password=NEO4J_PASSWORD,\n","        database='hnm',\n","        index_name='product-text-embeddings',\n","        retrieval_query=f\"\"\"\n","        WITH node AS product, score AS searchScore\n","\n","        OPTIONAL MATCH(product)<-[:VARIANT_OF]-(:Article)<-[:PURCHASED]-(:Customer)\n","        -[:PURCHASED]->(a:Article)<-[:PURCHASED]-(:Customer {{customerId: '{customer_id}'}})\n","        WITH count(a) AS purchaseScore, product, searchScore\n","        RETURN product.text + '\\nurl: ' + 'https://representative-domain/product/' + product.productCode  AS text,\n","            (1.0+purchaseScore)*searchScore AS score,\n","            {{source: 'https://representative-domain/product/' + product.productCode}} AS metadata\n","        ORDER BY purchaseScore DESC, searchScore DESC LIMIT 5\n","\n","    \"\"\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFAnXFV6dyXr"},"outputs":[],"source":["# Use the same tailored search recommendations as above but with a smaller limit\n","kg_recommendations_bot1 = Neo4jVector.from_existing_index(\n","    embedding=embedding_model,\n","    url=NEO4J_URI,\n","    username=NEO4J_USERNAME,\n","    password=NEO4J_PASSWORD,\n","    database='hnm',\n","    index_name='product-text-embeddings',\n","    retrieval_query=\"\"\"\n","    WITH node as searchProduct, score as searchScore\n","    MATCH(searchProduct)<-[:VARIANT_OF]-(:Article)-[r:CUSTOMERS_ALSO_LIKE]->(:Article)-[:VARIANT_OF]-(product)\n","    WITH  product, searchScore, sum(r.score*searchScore) AS recommenderScore\n","    RETURN product.text + '\\nurl: ' + 'https://representative-domain/product/' + product.productCode  AS text,\n","    recommenderScore AS score,\n","    {source: 'https://representative-domain/product/' + product.productCode} AS metadata\n","    ORDER BY score DESC LIMIT 5\n","    \"\"\"\n",")"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"i7sCt8roekeH"},"source":["### Prompt Engineering\n","Now let's define our prompts. We will combine two together:\n","1. A system prompt which, in this case tells the LLM how to generated the message\n","2. Human prompt: In this case just wraps the search prompt entered by the customer\n","\n","This will allow us to pass the customer search to the retrievers, but then also to the LLM for addition context when drafting the message."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUAROR6aekeI"},"outputs":[],"source":["general_system_template = '''\n","You are a personal assistant named Sally for a fashion, home, and beauty company called HRM.\n","write an email to {customerName}, one of your customers, to promote and summarize products relevant for them given the current season / time of year: {timeOfYear} .\n","Please only mention the Products listed below. Do not come up with or add any new products to the list.\n","Each product description comes with a \"url\" field. make sure to link to the url with descriptive name text for each product so the customer can easily find them.\n","\n","---\n","# Relevant Products:\n","{searchProds}\n","\n","# Customer May Also Be Interested In:\n","{recProds}\n","---\n","'''\n","general_user_template = \"{searchPrompt}\"\n","messages = [\n","    SystemMessagePromptTemplate.from_template(general_system_template),\n","    HumanMessagePromptTemplate.from_template(general_user_template),\n","]\n","prompt = ChatPromptTemplate.from_messages(messages)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"TgFbaUt6ekeI"},"source":["### Create a Chain\n","Now let's put a chain together that will leverage the retrievers, prompts, and LLM model. This is where Langchain shines, putting RAG together in a simple way.\n","\n","In addition to the personalized search and recommendations context, we will allow for som other parameters\n","\n","1. `customerName`: Ordinarily this will be pulled from Neo4j, but it has been scrubbed from the data for obvious reasons so we will provide our own name here.\n","2. `timeOfYear`: The time of year as a date, season, month, etc. the LLM can tailor the language appropriately.\n","\n","You can potentially add other creative parameters here to help the LLM write relevant messages.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nUpih07QdyXr"},"outputs":[],"source":["# Helper function\n","def format_docs(docs):\n","    return \"\\n\\n\".join([d.page_content for d in docs])\n","\n","def chain_gen(customer_id):\n","    return ({'searchProds': (lambda x:x['searchPrompt']) | kg_personalized_search_gen(customer_id).as_retriever(search_kwargs={\"k\": 100}) | format_docs,\n","              'recProds': (lambda x:x['searchPrompt']) | kg_recommendations_bot1.as_retriever(search_kwargs={\"k\": 5}) | format_docs,\n","              'customerName': lambda x:x['customerName'],\n","              'timeOfYear': lambda x:x['timeOfYear'],\n","              \"searchPrompt\":  lambda x:x['searchPrompt']}\n","             | prompt\n","             | llm\n","             | StrOutputParser())"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"JjbUGH6WekeI"},"source":["### Examples Runs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lkBdqOVjekeI"},"outputs":[],"source":["chain = chain_gen(CUSTOMER_ID)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jL6P3IoydyXr","outputId":"5efa77e2-efe2-4156-bd7f-28b88c170c18"},"outputs":[],"source":["print(chain.invoke({'searchPrompt':search_prompt, 'customerName':'Alex Smith', 'timeOfYear':'Dec, 2023'}))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2E50XJ_3dyXr","outputId":"ffaf712d-cfaa-4504-bee7-a7694648ec71"},"outputs":[],"source":["print(chain.invoke({'searchPrompt':\"western boots\", 'customerName':'Alex Smith', 'timeOfYear':'Dec, 2023'}))"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"8G_vdFviekeI"},"source":["Feel free to experiment and try more!"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"1IU_gedrekeI"},"source":["### Demo App\n","Now lets use the above tools to create a demo app with Gradio.  We will need to make a couple more functions, but otherwise easy to fire up from a Notebook!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1F0ve3cekeI"},"outputs":[],"source":["# Create a means to generate and cache chains...so we can quickly try different customer ids\n","personalized_search_chain_cache = dict()\n","def get_chain(customer_id):\n","    if customer_id in personalized_search_chain_cache:\n","        return personalized_search_chain_cache[customer_id]\n","    chain = chain_gen(customer_id)\n","    personalized_search_chain_cache[customer_id] = chain\n","    return chain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":648},"id":"XsBcFQLlekeI","outputId":"11dc80c9-b7f4-463d-ef81-9f54e5ac0825","pycharm":{"is_executing":true}},"outputs":[],"source":["import gradio as gr\n","\n","def message_generator(*x):\n","    chain = get_chain(x[0])\n","    return chain.invoke({'searchPrompt':x[3], 'customerName':x[2], 'timeOfYear': x[1]})\n","\n","customer_id = gr.Textbox(value=CUSTOMER_ID, label=\"Customer ID\")\n","time_of_year = gr.Textbox(value=\"Dec, 2023\", label=\"Time Of Year\")\n","search_prompt = gr.Textbox(value='Oversized Sweaters', label=\"Customer Interests(s)\")\n","customer_name = gr.Textbox(value='Alex Smith', label=\"Customer Name\")\n","message_result = gr.Markdown( label=\"Message\")\n","\n","demo = gr.Interface(fn=message_generator,\n","                    inputs=[customer_id, time_of_year, customer_name, search_prompt],\n","                    outputs=message_result,\n","                    title=\"ðŸª„ Message Generator ðŸ¥³\")\n","demo.launch(share=True, debug=True)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"KQyKH5YbekeI"},"source":["### Demo App - Directly to Recommendations\n","There are lots of different ways we can configure this.  Let's try a shorter version that cuts right to personalized recommendations and makes an in season pun."]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"qAgo8YSJekeI"},"source":["First we will create a function to retrieve based off our personalized recommendations example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ti6exWUpekeI"},"outputs":[],"source":["def kg_recommendations_app2(customer_id, k=30):\n","    res = kg.query(\"\"\"\n","    MATCH(:Customer {customerId:$customerId})-[:PURCHASED]->(:Article)\n","    -[r:CUSTOMERS_ALSO_LIKE]->(:Article)-[:VARIANT_OF]->(product)\n","    RETURN product.text + '\\nurl: ' + 'https://representative-domain/product/' + product.productCode  AS text,\n","        sum(r.score) AS recommenderScore\n","    ORDER BY recommenderScore DESC LIMIT $k\n","    \"\"\", params={'customerId': customer_id, 'k':k})\n","\n","    return \"\\n\\n\".join([d['text'] for d in res])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2TBYg6RZekeI","outputId":"5485ca2f-70fc-47c7-e712-83fb85e51b9d"},"outputs":[],"source":["# test out\n","print(kg_recommendations_app2(CUSTOMER_ID))"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"N60Wx4slekeI"},"source":["Next we re-define our prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FqlJ8IJbekeI"},"outputs":[],"source":["general_system_template_app2 = '''\n","You are a personal assistant named Sally for a fashion, home, and beauty company called HRM.\n","write an email to {customerName}, one of your customers, to promote and summarize products that fasionably pair with what they searched for given the current season / time of year: {timeOfYear}.\n","Make an in-season pun too!\n","Please only choose from the Products listed below. Choose no more than 5. Do not come up with or add any new products to the list.\n","Each product description comes with a \"url\" field. make sure to link to the url with descriptive name text for each product so the customer can easily find them.\n","\n","---\n","# Relevant Products:\n","{recProds}\n","---\n","'''\n","\n","general_user_template_app2 = '''Something that goes with {searchPrompt}'''\n","messages_app2 = [\n","    SystemMessagePromptTemplate.from_template(general_system_template_app2),\n","    HumanMessagePromptTemplate.from_template(general_user_template_app2),\n","]\n","prompt_app2 = ChatPromptTemplate.from_messages(messages_app2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h-cLWJ3uekeI"},"outputs":[],"source":["from operator import itemgetter\n","from langchain.schema.runnable import RunnableLambda\n","\n","chain_app2 = ({'recProds': itemgetter('customerId') |  RunnableLambda(kg_recommendations_app2),\n","             'customerName': lambda x:x['customerName'],\n","             'timeOfYear': lambda x:x['timeOfYear'],\n","             \"searchPrompt\":  lambda x:x['searchPrompt']}\n","            | prompt_app2\n","            | llm\n","            | StrOutputParser())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pMbwWuncekeJ","outputId":"192688dd-6d60-4553-e8d5-be9c5ad0c70c"},"outputs":[],"source":["print(chain_app2.invoke({'customerId':CUSTOMER_ID, 'searchPrompt':\"western boots\", 'customerName':'Alex Smith', 'timeOfYear':'Nov, 2023'}))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":612},"id":"2SJfNgEfekeJ","outputId":"c522813a-e9f1-4cd7-9663-1112fbfadd1f"},"outputs":[],"source":["import gradio as gr\n","\n","def message_generator_app2(*x):\n","    return chain_app2.invoke({'searchPrompt':x[3],\n","                              'customerName':x[2],\n","                              'timeOfYear': x[1],\n","                              'customerId': x[0]})\n","\n","customer_id = gr.Textbox(value=CUSTOMER_ID, label=\"Customer ID\")\n","time_of_year = gr.Textbox(value=\"Nov, 2023\", label=\"Time Of Year\")\n","customer_name = gr.Textbox(value='Alex Smith', label=\"Customer Name\")\n","search_prompt = gr.Textbox(value='Oversized Sweaters', label=\"Customer Interests(s)\")\n","message_result = gr.Markdown( label=\"Message\")\n","\n","demo = gr.Interface(fn=message_generator_app2,\n","                    inputs=[customer_id, time_of_year, customer_name, search_prompt],\n","                    outputs=message_result,\n","                    title=\"ðŸª„ Message Generator - Recommendations and Puns ðŸ¥³\")\n","demo.launch(share=True, debug=True)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"0KJYGJpAekeJ"},"source":["## Wrap Up"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDjWNPQXdyXr"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1FRMUyVmrDM5nlAjNYZPwgypZ-0Wrf82p","timestamp":1699873355703}],"toc_visible":true},"kernelspec":{"display_name":"multiMod_venv","language":"python","name":"multimod_venv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1276e5eefd774760ac26905b5b77c869":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1c59e75d027d47cb96343f1af89dd1b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e439939467c14b458d1d1fd8c5289696","placeholder":"â€‹","style":"IPY_MODEL_1276e5eefd774760ac26905b5b77c869","value":" 100.0/100 [00:26&lt;00:00, 37.78%/s]"}},"225085d2e10b470585295d2874fdddbd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d080ad2952e5484ba7a1d5d316b23937","IPY_MODEL_3cdc91c7a77d4e0d877c30497c69f207","IPY_MODEL_32657aa7bcaa4cc3aec4317a5a9cbae1"],"layout":"IPY_MODEL_fdf98d0b55e649b9882f530e299b76a7"}},"32657aa7bcaa4cc3aec4317a5a9cbae1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8deecfeaf21444d69725d14ab763805c","placeholder":"â€‹","style":"IPY_MODEL_443c064fee954412a928de68c37fd7d0","value":" 100.0/100 [02:54&lt;00:00,  4.89%/s]"}},"3cdc91c7a77d4e0d877c30497c69f207":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f29d15f0ee2b41fcb92f80b7fa2ea944","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e920f4310fcf422dbe9300db0bd361d7","value":100}},"3e83412ca7c1496096af0ec386ff7aac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"409783c7087440c980e05fb9a33cca74":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43ee75db1a1a486bb944c7931f5d9160":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e83412ca7c1496096af0ec386ff7aac","placeholder":"â€‹","style":"IPY_MODEL_be48419a3ad34da6877b78554f009bc4","value":"FastRP: 100%"}},"443c064fee954412a928de68c37fd7d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b5a261ce52649759538b06b1d00ec0d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_43ee75db1a1a486bb944c7931f5d9160","IPY_MODEL_ef3ac03f92b44465a26d51cd62d816eb","IPY_MODEL_1c59e75d027d47cb96343f1af89dd1b1"],"layout":"IPY_MODEL_65dbcea2dfba4bd7be50a9e76b65d086"}},"65dbcea2dfba4bd7be50a9e76b65d086":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87e17419744f42eb9ecbf995ec5cde4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8deecfeaf21444d69725d14ab763805c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be48419a3ad34da6877b78554f009bc4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d080ad2952e5484ba7a1d5d316b23937":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d35230e08e8a4cec97bcbbd603d660a5","placeholder":"â€‹","style":"IPY_MODEL_87e17419744f42eb9ecbf995ec5cde4c","value":"Knn: 100%"}},"d35230e08e8a4cec97bcbbd603d660a5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc033bf8a9b44b2692051ecd0c980881":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e439939467c14b458d1d1fd8c5289696":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e920f4310fcf422dbe9300db0bd361d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef3ac03f92b44465a26d51cd62d816eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_409783c7087440c980e05fb9a33cca74","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dc033bf8a9b44b2692051ecd0c980881","value":100}},"f29d15f0ee2b41fcb92f80b7fa2ea944":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdf98d0b55e649b9882f530e299b76a7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
